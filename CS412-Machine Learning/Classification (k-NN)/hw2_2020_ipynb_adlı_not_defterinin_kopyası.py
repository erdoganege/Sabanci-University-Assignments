# -*- coding: utf-8 -*-
"""HW2-2020.ipynb adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ej0dfRkMVySusQRC7eSjxf6QJBY83Cfz

# CS412 - Machine Learning - 2020

# Homework 2

100 pts

# Goal

The goal of this homework is to get familiar feature handling and cross validation.


# Dataset

German Credit Risk dataset, prepared by Prof. Hoffman, classifies each person as having a good or bad credit risk. The dataset that we use consists of both numerical and categorical features.



# Task

Build a k-NN classifier with scikit-learn library to classify people as bad or good risks for the german credit dataset. 

# Software

Documentation for the necessary functions can be accessed from the link below.

[http://scikit-learn.org/stable/supervised_learning.html](http://scikit-learn.org/stable/supervised_learning.html)

# Submission

Follow the instructions at the end.

# 1) Initialize

First, make a copy of this notebook in your drive
"""

# Mount to your drive, in this way you can reach files that are in your drive
# Run this cell
# Go through the link that will be showed below
# Select your google drive account and copy authorization code and paste here in output and press enter
# You can also follow the steps from that link
# https://medium.com/ml-book/simplest-way-to-open-files-from-google-drive-in-google-colab-fae14810674 

from google.colab import drive
drive.mount('/content/drive')

"""# 2) Load Dataset

To start working for your homework, take a copy of the folder, given in the below link to your own google drive. You find the train and test data under this folder.

[https://drive.google.com/drive/folders/1DbW6VxLKZv2oqFn9SwxAnVadmn1_nPXi?usp=sharing](https://drive.google.com/drive/folders/1DbW6VxLKZv2oqFn9SwxAnVadmn1_nPXi?usp=sharing)

After copy the folder, copy the path of the train and test dataset to paste them in the below cell to load your data.

"""

import pandas as pd

train_df = pd.read_csv('/content/drive/My Drive/Copy of german_credit_train.csv')
test_df = pd.read_csv('/content/drive/My Drive/Copy of german_credit_test.csv')

"""# 3) Optional - Analyze the Dataset 

You can use the functions of the pandas library to analyze your train dataset in detail - **this part is OPTIONAL - look around the data as you wish**.


*   Display the number of instances and features in the train ***(shape function can be used)**
*   Display 5 random examples from the train ***(sample function can be used)**
*   Display the information about each features ***(info method can be used)**


"""

# Print shape
print("Train data dimensionality: ", train_df.shape)

# Print random 5 rows
print("Examples from train data: ")
train_df.sample(n = 5)

# Print the information about the dataset
print("Information about train data ", train_df.info())

"""# 4) Define your train and test labels

*  Define labels for both train and test data in new arrays 
*  And remove the label column from both train and test sets do tht it is not used as a feature! 


(**you can use pop method**)

"""

# Define labels
train_label = train_df.pop("Risk")
test_label = test_df.pop("Risk")

train_label

"""# 5) Handle missing values if any 

*   Print the columns that have **NaN** values (**isnull** method can be used)
*   You can impute missing values with mode of that feature or remove samples or attributes
*   To impute the test set, you should use the mode values that you obtain from **train** set, as **you should not be looking at your test data to gain any information or advantage.**


"""

# Print columns with NaN values
print(train_df.isnull().any())
print(train_df.isnull().sum())

print(train_df["Housing"])
train_df["Housing"].value_counts()

# Impute missing values by replacing with mode value
train_df["Housing"] = train_df["Housing"].fillna(train_df["Housing"].mode()[0])
test_df["Housing"] = test_df["Housing"].fillna(train_df["Housing"].mode()[0])
print(train_df.isnull().sum())
print(test_df.isnull().sum())

"""# 6) Transform categorical / ordinal features

* Transform all categorical / ordinal features using the methods that you have learnt in lectures and recitation 4 for both train and test data
* You saw the dictionary use for mapping in recitation. (You can use **replace function** to assign new values to the categories of a column).

*  The class of the categorical attributes in the dataset are defined as follows:
  - Status of existing checking account
     - A11 :      ... <    0 DM
	- A12 : 0 <= ... <  200 DM
	- A13 :      ... >= 200 DM / salary assignments for at least 1 year
     - A14 : no checking account

 - Credit history
    - A30 : no credits taken/all credits paid back duly
    - A31 : all credits at this bank paid back duly
	- A32 : existing credits paid back duly till now
    - A33 : delay in paying off in the past
	- A34 : critical account/other credits existing (not at this bank)

  - Savings account
    - A61 :          ... <  100 DM
	- A62 :   100 <= ... <  500 DM
	- A63 :   500 <= ... < 1000 DM
	- A64 :          .. >= 1000 DM
    - A65 :   unknown/ no savings account

 - Employment Since
    - A71 : unemployed
    - A72 :       ... < 1 year
	- A73 : 1  <= ... < 4 years  
	- A74 : 4  <= ... < 7 years
	- A75 :       .. >= 7 years
 
 - Personal Status
    - A91 : male   : divorced/separated
	- A92 : female : divorced/separated/married
    - A93 : male   : single
	- A94 : male   : married/widowed
	- A95 : female : single

  - Property
     -  A121 : real estate
	- A122 : if not A121 : building society savings agreement/life insurance
    - A123 : if not A121/A122 : car or other, not in attribute 6
	- A124 : unknown / no property

 - OtherInstallPlans  
    - A141 : bank
	- A142 : stores
	- A143 : none

 - Housing
    -  A151 : rent
	 - A152 : own
	- A153 : for free
"""

train_df.head()

# Transform the categorical / ordinal attributes
print(train_df["AccountStatus"].unique())
AccountStatusMap = {"A14":0, "A11":1, "A12":2, "A13":3}
train_df["AccountStatus"] = train_df["AccountStatus"].replace(AccountStatusMap)
train_df.head()

print(train_df["CreditHistory"].unique())
CreditHistoryMap = {"A34":0, "A33":1, "A32":2, "A31":3, "A30":4}
train_df["CreditHistory"] = train_df["CreditHistory"].replace(CreditHistoryMap)
train_df.head()

print(train_df["SavingsAccount"].unique())
SavingsAccountMap = {"A65":0,"A61":1,"A62":2,"A63":3, "A64":4}
train_df["SavingsAccount"] = train_df["SavingsAccount"].replace(SavingsAccountMap)
train_df.head()

print(train_df["EmploymentSince"].unique())
EmploymentSinceMap = {"A71":0,"A72":1,"A73":2,"A74":3, "A75":4}
train_df["EmploymentSince"] = train_df["EmploymentSince"].replace(EmploymentSinceMap)
train_df.head()

print(train_df["PersonalStatus"].unique())
dummies_status = pd.get_dummies(train_df['PersonalStatus'],prefix='status')
dummies_status.head()

train_df = pd.merge(train_df,dummies_status,left_index=True,right_index=True)
train_df = train_df.drop(columns="PersonalStatus")
train_df.head()

print(train_df["Property"].unique())
dummies_property = pd.get_dummies(train_df["Property"],prefix='property')
dummies_property.head()

train_df = pd.merge(train_df,dummies_property,left_index=True,right_index=True)
train_df = train_df.drop(columns="Property")
train_df.head()

print(train_df["OtherInstallPlans"].unique())
dummies_plans = pd.get_dummies(train_df["OtherInstallPlans"],prefix='plans')
dummies_plans.head()

train_df = pd.merge(train_df,dummies_plans,left_index=True,right_index=True)
train_df = train_df.drop(columns="OtherInstallPlans")
train_df.head()

print(train_df["Housing"].unique())
dummies_housing = pd.get_dummies(train_df["Housing"],prefix='plans')
dummies_housing.head()

train_df = pd.merge(train_df,dummies_housing,left_index=True,right_index=True)
train_df = train_df.drop(columns="Housing")
train_df.head()

print(test_df["AccountStatus"].unique())
AccountStatusMap = {"A14":0, "A11":1, "A12":2, "A13":3}
test_df["AccountStatus"] = test_df["AccountStatus"].replace(AccountStatusMap)

print(test_df["CreditHistory"].unique())
CreditHistoryMap = {"A34":0, "A33":1, "A32":2, "A31":3, "A30":4}
test_df["CreditHistory"] = test_df["CreditHistory"].replace(CreditHistoryMap)

print(test_df["SavingsAccount"].unique())
SavingsAccountMap = {"A65":0,"A61":1,"A62":2,"A63":3, "A64":4}
test_df["SavingsAccount"] = test_df["SavingsAccount"].replace(SavingsAccountMap)
print(test_df["EmploymentSince"].unique())
EmploymentSinceMap = {"A71":0,"A72":1,"A73":2,"A74":3, "A75":4}
test_df["EmploymentSince"] = test_df["EmploymentSince"].replace(EmploymentSinceMap)

print(test_df["PersonalStatus"].unique())
dummies_status = pd.get_dummies(test_df['PersonalStatus'],prefix='status')
test_df = pd.merge(test_df,dummies_status,left_index=True,right_index=True)
test_df = test_df.drop(columns="PersonalStatus")

print(test_df["Property"].unique())
dummies_property = pd.get_dummies(test_df["Property"],prefix='property')
test_df = pd.merge(test_df,dummies_property,left_index=True,right_index=True)
test_df = test_df.drop(columns="Property")

print(test_df["OtherInstallPlans"].unique())
dummies_plans = pd.get_dummies(test_df["OtherInstallPlans"],prefix='plans')
test_df = pd.merge(test_df,dummies_plans,left_index=True,right_index=True)
test_df = test_df.drop(columns="OtherInstallPlans")

print(test_df["Housing"].unique())
dummies_housing = pd.get_dummies(test_df["Housing"],prefix='plans')
test_df = pd.merge(test_df,dummies_housing,left_index=True,right_index=True)
test_df = test_df.drop(columns="Housing")

test_df.head()

"""# 7) Build a k-NN classifier on training data and perform models selection using 5 fold cross validation

*  Initialize k-NN classifiers with **k= 5, 10, 15**
*  Calculate the cross validation scores using cross_al_score method, number of folds is 5. 
*  Note: Xval is performed on training data! Do not use test data in any way and do not separate a hold-out validation set, rather use cross-validation.

Documentation of the cross_val_score method:

[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)

*  Stores the average accuracies of these folds
*  Select the value of k using the cross validation results. 
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from statistics import mean

# k values
kVals = [5,10,15]

# Save the accuracies of each value of kVal in [accuracies] variable
accuracies = []

# Loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  # Initialize a k-NN classifier with k neighbors
  my_classifier = KNeighborsClassifier(n_neighbors= k)
  
  # Calculate the 5 fold cross validation scores using cross_val_score
  # cv parameter: number of folds, in our case it must be 5
  scores = cross_val_score(my_classifier, train_df, train_label, cv = 5)
  # Stores the average accuracies of the scores in accuracies variable, you can use mean method
  accuracies.append(mean(scores))

print(accuracies)

"""# 8) Retrain using all training data and test on test set

* Train a classifier with the chosen k value of the best classifier using **all training data**. 

Note:  k-NN training involves no explicit training, but this is what we would do after model selection with decision trees or any other ML approach (we had 5 diff. models -one for each fold - for each k in the previous step - dont know which one to submit. Even if we picked the best one, it does not use all training samples.

* Predict the labels of testing data 

* Report the accuracy 
"""

from sklearn.metrics import accuracy_score

# Train the best classifier using all training set
my_best_classifier = KNeighborsClassifier(n_neighbors= 15)
my_best_classifier.fit(train_df, train_label)
# Estimate the prediction of the test data
pred = my_best_classifier.predict(test_df)

# Print accuracy of test data
accuracy = accuracy_score(test_label, pred)
print(accuracy)

"""# 9) Bonus (5pts)

There is a limited bonus for any extra work that you may use and improve the above results. 

You may try a larger k values, scale input features, remove some features, .... Please **do not overdo**, maybe spend another 30-60min on this. The idea is not do an exhaustive search (which wont help your understanding of ML process), but just to give some extra points to those who may look at the problem a little more comprehensively. 

**If you obtain better results than the above, please indicate the best model you have found and the corresponding accuracy.**

E.g. using feature normalization ..... and removing .... features and using a value k=...., I have obtained ....% accuracy.

"""

train_df.head()

train_label.head()

#Make label as binary zeros and ones
Label = {1:1, 2:0}
train_label = train_label.replace(Label)
test_label = test_label.replace(Label)
train_label.head()

#Normalization of datasets
train_df = (train_df - train_df.min())/(train_df.max()-train_df.min())
test_df = (test_df - test_df.min())/(test_df.max()-test_df.min())
train_df.head()

#Drop PERSONAL STATUS 
train_df.pop("status_A91")
train_df.pop("status_A92")
train_df.pop("status_A93")
train_df.pop("status_A94")

test_df.pop("status_A91")
test_df.pop("status_A92")
test_df.pop("status_A93")
test_df.pop("status_A94")

#Drop OtherInstallPlans
train_df.pop("plans_A141")
train_df.pop("plans_A142")
train_df.pop("plans_A143")

test_df.pop("plans_A141")
test_df.pop("plans_A142")
test_df.pop("plans_A143")

# k values
kVals = [5,10,15, 20]

# Save the accuracies of each value of kVal in [accuracies] variable
accuracies = []

# Loop over values of k for the k-Nearest Neighbor classifier
for k in kVals:
  # Initialize a k-NN classifier with k neighbors
  my_classifier = KNeighborsClassifier(n_neighbors= k)
  
  # Calculate the 5 fold cross validation scores using cross_val_score
  # cv parameter: number of folds, in our case it must be 5
  scores = cross_val_score(my_classifier, train_df, train_label, cv = 5)
  # Stores the average accuracies of the scores in accuracies variable, you can use mean method
  accuracies.append(mean(scores))

print(accuracies)

deneme = KNeighborsClassifier(n_neighbors= 5)
deneme.fit(train_df, train_label)
# Estimate the prediction of the test data
new_pred = deneme.predict(test_df)

# Print accuracy of test data
accuracy = accuracy_score(test_label, new_pred)
print(accuracy)

"""# 10) Notebook & Report

**Notebook:** We may just look at your notebook results; so make sure each cell is run and outputs are there.

**Report:** Write an at most 1/2 page summary of your approach to this problem at the end of your notebook; this should be like an abstract of a paper or the executive summary.

**Must include statements such as:**

( Include the problem definition: 1-2 lines )

(Talk about any preprocessing you do, How you handle missing values and categorical features)

( Give the average validation accuracies for different k values and standard deviations between 5 folds of each k values, state which one you selected)

( State what your test results are with the chosen method, parameters: e.g. "We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data….""

State if there is any **bonus** work...

You will get full points from here as long as you have a good (enough) summary of your work, regardless of your best performance or what you have decided to talk about in the last few lines.

# 11) Submission

Please submit your **"share link" INLINE in Sucourse submissions**. That is we should be able to click on the link and go there and run (and possibly also modify) your code. 

For us to be able to modify, in case of errors etc, **you should get your "share link" as **share with anyone in edit mode** 

 **Also submit your notebook as pdf as attachment**, choose print and save as PDF, save with hw2-lastname-firstname.pdf to facilitate grading.

<h1><b>REPORT</b></h1>
Our task is building a k-NN classifier to classify people as bad or good risks for the german credit dataset in this assignment. <b>Firstly</b>, I loaded the datasets as training and test data. Then, I analyze the dataset by using ".shape(), .info() functions". I realized that there are both <b>numerical and categorical features</b>. Thus, I need to play these features to make them proper for our k-NN classifier. Before going that<b> in part 4</b> I need to extract these labels from training and test data before training our classifier. <b>In part 5</b>, I checked our training data if it includes any missing value and I realized that there is a column <b>"Housing"</b> with missing values. I fill the missing values of training and test set with <b>mode of training data for both of them</b>. <b>In part 6</b>, I transformed categorical / ordinal features for appropriate way. I chose "Status of existing checking account", "Credit history", "Savings account", "Employment Since" as <b>ordinal features</b> so I gave them "orders" by using .replace() and dictionaries. On the other hand, I chose "Personal Status", "Property", "OtherInstallPlans", "Housing" as <b>Categorical features</b> and I used .getdummies() function. <b>In part 7,</b> I trained my model by using 5 fold cross validation with different k values and I recorded accuracies for each different k value. <b>The result table is below</b>:
<h3><b>Accuracy table for different k values</b></h3> 
<table>
    <th>k value</th>
    <th>Accuracy </th>
    <tr>
      <td>5</td>
      <td>0.675</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.7075</td>
    </tr>
    <tr>
      <td><b>15</b></td>
      <td><b>0.71</b></td>
    </tr>
</table>
I chose the <b>k = 15</b> for testing my model. Before testing I trained my model again with my entire training set with k = 15. Then, I tested my model. The result:
<table>
    <th>k value</th>
    <th>Accuracy </th>
    <tr>
      <td>15</td>
      <td>0.665</td>
    </tr>
</table>
<b>In Bonus Part</b>, Firstly, I decided to "Personal Status" and "OtherInstallingPlans" are not related to credit card risk so, I removed these features from my training and test data. I also modified my label sets as binary classification "1" and "0" by using .replace() function. Then, I used <b>normalization</b> on my real numeric values to fit between 0 and 1. Then, I tried several different k values and I chose <b>k as 5. The result:</b>
<table>
    <th>k value</th>
    <th>Accuracy </th>
    <tr>
      <td>5</td>
      <td>0.705</td>
    </tr>
</table>
"""